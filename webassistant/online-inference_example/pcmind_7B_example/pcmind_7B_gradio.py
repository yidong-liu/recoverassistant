"""
åœ¨çº¿æ¨ç†ä»»åŠ¡é…ç½®ï¼š
é›†ç¾¤ï¼šæ™ºç®—GPU
èµ„æºï¼šA100,V100
é•œåƒï¼šLlama3ï¼ˆchenzhåˆ›å»ºï¼‰
æ¨¡å‹ï¼š (ä»“åº“)
å¯åŠ¨è„šæœ¬:
"""

import os
import gradio as gr
from c2net.context import prepare

c2net_context = prepare()
model_path = (
    c2net_context.pretrain_model_path
    + "/PCMind-7B-1T-Transformers"
)

from fastapi import FastAPI
from dataclasses import dataclass, field
from threading import Thread
from typing import Iterable, List, Tuple
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer

import sys

sys.path.append(
    "/tmp/pretrainmodel/PCMind-7B-1T-Transformers"
)

app = FastAPI()
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_path, device_map="auto", trust_remote_code=True
)
terminators = [tokenizer.eos_token_id]

DESCRIPTION = f"""
<div>
<h1 style="text-align: center;">PengCheng-Mind 7B åœ¨çº¿ä½“éªŒ</h1>
<p>ğŸ” æ­¤ä½“éªŒç•Œé¢é€šè¿‡å¯æ™ºç¤¾åŒºåœ¨çº¿æ¨ç†ä»»åŠ¡æ„å»ºï¼Œä½¿ç”¨ç®—åŠ›ç½‘èµ„æºéƒ¨ç½²ã€‚</p>
<p>âš™ï¸ å¯åœ¨ä¸‹æ–¹ `Parameters` ä¸­è®¾ç½®å‚æ•°ï¼Œè°ƒæ•´æ¨¡å‹è¾“å‡ºæ•ˆæœï¼š<br>
    1> `Tempurature` å€¼è¶Šå¤§ï¼Œæ¨¡å‹è¾“å‡ºä¸ç¡®å®šæ€§è¶Šé«˜ï¼›<br>
    2> `Max new tokens` æ§åˆ¶æ¨¡å‹æœ€å¤§å¯ç”Ÿæˆé•¿åº¦ï¼›<br>
    3> `Top p` å€¼è¶Šå¤§ï¼Œæ¨¡å‹è¾“å‡ºå¤šæ ·æ€§å’Œéšæœºæ€§ï¼›<br>
    4> `Repetition Penalty` å€¼è¶Šå¤§ï¼Œæ¨¡å‹é‡å¤åŒæ ·å†…å®¹çš„æ¦‚ç‡è¶Šä½ï¼›<br>
    5> `System Message` ç³»ç»Ÿé¢„è®¾æŒ‡ä»¤ï¼Œä½ å¯ä»¥è®¾å®šå¤§æ¨¡å‹çš„äººè®¾ </p></p>
</div>
"""

LICENSE = """
<p/>

---
Built with OpenI and C2Net.
"""

PLACEHOLDER = f"""
<div style="padding: 30px; text-align: center; display: flex; flex-direction: column; align-items: center;">
   <h1 style="font-size: 28px; margin-bottom: 2px; opacity: 0.55;">PengCheng-Mind 7B</h1>
   <p style="font-size: 18px; margin-bottom: 2px; opacity: 0.65;">è¯·æé—®ï¼Œæ¨¡å‹æ”¯æŒ <b>ä¸­æ–‡</b> ä¸ <b>è‹±æ–‡</b> è¾“å…¥<br>
</div>
"""

css = """
h1 {
  text-align: center;
  display: block;
}s
#duplicate-button {
  margin: auto;
  color: white;
  background: #1565c0;
  border-radius: 100vh;
}
"""


def streaming_chat(
    message: str,
    history: List[Tuple[str, str]] = [],
    system_message: str = "ä½ æ˜¯ä¸€ä¸ªèŠå¤©åŠ©æ‰‹ï¼Œä½ éœ€è¦ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜!",
    max_tokens: int = 512,
    temperature: float = 0.7,
    top_p: float = 0.9,
    repetition_penalty: float = 1.0,
) -> Iterable[str]:
    """
    Generate a streaming response using the llama3-8b model.
    """
    conversation = [{"role": "system", "content": system_message}]
    if history is not None:
        for user, assistant in history:
            conversation.extend(
                [
                    {"role": "Human", "content": user},
                    {"role": "Assistant", "content": assistant},
                ]
            )
    conversation.append({"role": "Human", "content": message})
    input_str = ""
    for message_dict in conversation:
        input_str += message_dict["role"] + ": " + message_dict["content"] + "\n "

    """    
    input_ids = tokenizer.apply_chat_template(conversation, return_tensors="pt", add_special_tokens=False).to(
        model.device
    )
    """
    input_ids = tokenizer.encode(
        input_str, return_tensors="pt", add_special_tokens=False
    ).to(model.device)

    streamer = TextIteratorStreamer(
        tokenizer, timeout=10.0, skip_prompt=True, skip_special_tokens=True
    )

    generate_kwargs = dict(
        input_ids=input_ids,
        streamer=streamer,
        do_sample=True,
        max_new_tokens=max_tokens,
        temperature=temperature,
        top_p=top_p,
        repetition_penalty=repetition_penalty,
        eos_token_id=terminators,
    )
    # This will enforce greedy generation (do_sample=False) when the temperature is passed 0, avoiding the crash.
    if temperature == 0:
        generate_kwargs["do_sample"] = False

    t = Thread(target=model.generate, kwargs=generate_kwargs)
    t.start()

    response = ""
    for new_token in streamer:
        if new_token != "":
            response += new_token
            yield response


# Gradio block
chatbot = gr.Chatbot(height=450, placeholder=PLACEHOLDER, label="Gradio ChatInterface")

with gr.Blocks(fill_height=True, css=css) as demo:

    gr.Markdown(DESCRIPTION)

    gr.ChatInterface(
        fn=streaming_chat,
        chatbot=chatbot,
        fill_height=True,
        additional_inputs_accordion=gr.Accordion(
            label="âš™ï¸ Parameters", open=False, render=False
        ),
        additional_inputs=[
            gr.Textbox(
                value="ä½ æ˜¯ä¸€ä¸ªèŠå¤©åŠ©æ‰‹ï¼Œä½ éœ€è¦ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜!",
                label="System message",
            ),
            gr.Slider(
                minimum=1, maximum=2048, value=512, step=1, label="Max new tokens"
            ),
            gr.Slider(
                minimum=0.1, maximum=4.0, value=0.7, step=0.1, label="Temperature"
            ),
            gr.Slider(
                minimum=0.1,
                maximum=1.0,
                value=0.95,
                step=0.05,
                label="Top-p",
            ),
            gr.Slider(
                minimum=0.1,
                maximum=1.0,
                value=0.95,
                step=0.05,
                label="Repetition Penalty",
            ),
        ],
        examples=[
            ["å¦‚ä½•åœ¨ç«æ˜Ÿä¸Šå»ºç«‹äººç±»åŸºåœ°ï¼Ÿç®€çŸ­å›ç­”ã€‚"],
            ["å‘8å²çš„æˆ‘è§£é‡Šç›¸å¯¹è®ºã€‚"],
            ["9,000ä¹˜ä»¥9,000æ˜¯å¤šå°‘ï¼Ÿ"],
            ["ç»™æˆ‘çš„æœ‹å‹éŸ©æ¢…æ¢…å†™ä¸€æ®µæ¸©é¦¨çš„ç”Ÿæ—¥ç¥ç¦ã€‚"],
            ["è¾©è¯ä¸ºä»€ä¹ˆå°ä¼é¹…å¯èƒ½é€‚åˆå½“æ£®æ—ä¹‹ç‹ï¼Ÿ"],
        ],
        cache_examples=False,
    )

    gr.Markdown(LICENSE)

demo.queue()
app = gr.mount_gradio_app(
    app,
    demo,
    path=os.getenv("OPENI_SELF_URL"),
    root_path=os.getenv("OPENI_SELF_URL"),
)

import uvicorn

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=int(os.getenv('OPENI_SELF_PORT')))